{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Selected 1000 images for category 'person'\n",
      "Selected 1000 images for category 'cat'\n",
      "Selected 1000 images for category 'dog'\n",
      "Total selected images: 2972\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Selected 200 images for category 'person'\n",
      "Selected 184 images for category 'cat'\n",
      "Selected 177 images for category 'dog'\n",
      "Total selected images: 544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to C:\\Users\\lauro/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
      "100%|██████████| 167M/167M [00:25<00:00, 6.89MB/s] \n",
      "Epoch 1/10:   8%|▊         | 115/1486 [07:37<1:30:54,  3.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 274\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining completed. Final mAP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmAP\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 274\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 260\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    257\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# 3 classes + background\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Calculate mAP\u001b[39;00m\n\u001b[0;32m    263\u001b[0m mAP \u001b[38;5;241m=\u001b[39m calculate_map(model, val_loader)\n",
      "Cell \u001b[1;32mIn[1], line 134\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m    131\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    133\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 134\u001b[0m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    137\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths\n",
    "# TRAIN_PATH = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n",
    "# VAL_PATH = '/kaggle/input/coco-2017-dataset/coco2017/val2017'\n",
    "# ANNOTATIONS_PATH = '/kaggle/input/coco-2017-dataset/coco2017/annotations'\n",
    "# WORKING_DIR = '/kaggle/working'\n",
    "\n",
    "TRAIN_PATH = 'D:/Download/JDownloader/MSCOCO/images/train2017'\n",
    "VAL_PATH = 'D:/Download/JDownloader/MSCOCO/images/val2017'\n",
    "ANNOTATIONS_PATH = 'D:/Download/JDownloader/MSCOCO/annotations'\n",
    "WORKING_DIR = 'D:/Projetos/Mestrado/2024_Topicos_Esp_Sist_Informacao/ARTIGO_FINAL/object_detection_model_compare/working'\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transform=None, categories=['person', 'cat', 'dog'], samples_per_category=1000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "        \n",
    "        # Get category IDs\n",
    "        cat_ids = self.coco.getCatIds(catNms=categories)\n",
    "        self.category_mapping = {old_id: new_id + 1 for new_id, old_id in enumerate(cat_ids)}\n",
    "        \n",
    "        # Get exactly samples_per_category images per category\n",
    "        self.image_ids = self._select_balanced_images(cat_ids, samples_per_category)\n",
    "        print(f\"Total selected images: {len(self.image_ids)}\")\n",
    "        \n",
    "    def _select_balanced_images(self, cat_ids, samples_per_category):\n",
    "        \"\"\"Select exactly samples_per_category images for each category.\"\"\"\n",
    "        selected_images = []\n",
    "        category_counts = defaultdict(int)\n",
    "        \n",
    "        # Get all images with their categories\n",
    "        for cat_id in cat_ids:\n",
    "            img_ids = self.coco.getImgIds(catIds=[cat_id])\n",
    "            np.random.shuffle(img_ids)  # Randomize the order\n",
    "            \n",
    "            # Keep track of selected images for each category\n",
    "            for img_id in img_ids:\n",
    "                if category_counts[cat_id] < samples_per_category:\n",
    "                    selected_images.append(img_id)\n",
    "                    category_counts[cat_id] += 1\n",
    "        \n",
    "        # Print statistics\n",
    "        for cat_id in cat_ids:\n",
    "            cat_name = self.coco.loadCats([cat_id])[0]['name']\n",
    "            print(f\"Selected {category_counts[cat_id]} images for category '{cat_name}'\")\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        return list(dict.fromkeys(selected_images))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(os.path.join(self.root_dir, img_info['file_name'])).convert('RGB')\n",
    "        image = torchvision.transforms.ToTensor()(image)\n",
    "        \n",
    "        # Get annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in annotations:\n",
    "            if ann['category_id'] in self.category_mapping:\n",
    "                x, y, w, h = ann['bbox']\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                labels.append(self.category_mapping[ann['category_id']])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "def get_model(num_classes):\n",
    "    # Load pre-trained model\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "    \n",
    "    # Replace the classifier with a new one for our number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for images, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += losses.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += losses.item()\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f'Epoch {epoch+1} - Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def calculate_map(model, data_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            predictions = model(images)\n",
    "            \n",
    "            for pred, target in zip(predictions, targets):\n",
    "                all_predictions.append(pred)\n",
    "                all_targets.append(target)\n",
    "    \n",
    "    # Calculate mAP\n",
    "    aps = []\n",
    "    for class_id in range(1, 4):  # 3 classes + background\n",
    "        predictions_class = [pred['boxes'][pred['labels'] == class_id] for pred in all_predictions]\n",
    "        targets_class = [target['boxes'][target['labels'] == class_id] for target in all_targets]\n",
    "        \n",
    "        ap = calculate_ap(predictions_class, targets_class)\n",
    "        aps.append(ap)\n",
    "    \n",
    "    return np.mean(aps)\n",
    "\n",
    "def calculate_ap(predictions, targets, iou_threshold=0.5):\n",
    "    # Simplified AP calculation\n",
    "    if len(predictions) == 0 or len(targets) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for pred_boxes, target_boxes in zip(predictions, targets):\n",
    "        if len(pred_boxes) == 0 or len(target_boxes) == 0:\n",
    "            continue\n",
    "            \n",
    "        ious = box_iou(pred_boxes, target_boxes)\n",
    "        max_ious = torch.max(ious, dim=1)[0]\n",
    "        \n",
    "        tp += torch.sum(max_ious >= iou_threshold).item()\n",
    "        fp += torch.sum(max_ious < iou_threshold).item()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    return precision\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    \n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    \n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    \n",
    "    union = area1[:, None] + area2 - inter\n",
    "    \n",
    "    return inter / union\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, mAP, save_dir):\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'losses.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot mAP\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(mAP)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mAP')\n",
    "    plt.title('Mean Average Precision')\n",
    "    plt.savefig(os.path.join(save_dir, 'map.png'))\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Create datasets with exactly 1000 images per category\n",
    "    train_dataset = COCODataset(TRAIN_PATH, \n",
    "                               os.path.join(ANNOTATIONS_PATH, 'instances_train2017.json'),\n",
    "                               samples_per_category=1000)\n",
    "    val_dataset = COCODataset(VAL_PATH,\n",
    "                             os.path.join(ANNOTATIONS_PATH, 'instances_val2017.json'),\n",
    "                             samples_per_category=200)  # 20% of training size\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = get_model(num_classes=4)  # 3 classes + background\n",
    "    \n",
    "    # Train model\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader)\n",
    "    \n",
    "    # Calculate mAP\n",
    "    mAP = calculate_map(model, val_loader)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), os.path.join(WORKING_DIR, 'faster_rcnn_model.pth'))\n",
    "    \n",
    "    # Plot and save metrics\n",
    "    plot_metrics(train_losses, val_losses, [mAP], WORKING_DIR)\n",
    "    \n",
    "    print(f'Training completed. Final mAP: {mAP:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6332084,
     "sourceId": 10239510,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
