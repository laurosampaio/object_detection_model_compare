{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Creating datasets...\n",
      "loading annotations into memory...\n",
      "Done (t=8.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Found 1000 images\n",
      "Category mapping: {1: 1, 17: 2, 18: 3}\n",
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Found 1000 images\n",
      "Category mapping: {1: 1, 17: 2, 18: 3}\n",
      "Training dataset size: 1000\n",
      "Validation dataset size: 1000\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\lauro/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:18<00:00, 5.63MB/s]\n",
      "C:\\Users\\lauro\\AppData\\Local\\Temp\\ipykernel_27220\\3557294576.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from D:/Projetos/Mestrado/2024_Topicos_Esp_Sist_Informacao/ARTIGO_FINAL/object_detection_model_compare/weigths/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  63%|██████▎   | 317/500 [03:39<02:06,  1.44it/s, loss=0.401] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 293\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 293\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 278\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 278\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Calculate mAP\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating mAP...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 158\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, device, num_epochs)\u001b[0m\n\u001b[0;32m    155\u001b[0m         losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    156\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 158\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: losses\u001b[38;5;241m.\u001b[39mitem()})\n\u001b[0;32m    161\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "# TRAIN_PATH = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n",
    "# VAL_PATH = '/kaggle/input/coco-2017-dataset/coco2017/val2017'\n",
    "# ANNOTATIONS_PATH = '/kaggle/input/coco-2017-dataset/coco2017/annotations'\n",
    "# WORKING_DIR = '/kaggle/working'\n",
    "# WEIGHTS_PATH = '/kaggle/input/model-cache/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'\n",
    "\n",
    "\n",
    "TRAIN_PATH = 'D:/Download/JDownloader/MSCOCO/images/train2017'\n",
    "VAL_PATH = 'D:/Download/JDownloader/MSCOCO/images/val2017'\n",
    "ANNOTATIONS_PATH = 'D:/Download/JDownloader/MSCOCO/annotations'\n",
    "WORKING_DIR = 'D:/Projetos/Mestrado/2024_Topicos_Esp_Sist_Informacao/ARTIGO_FINAL/object_detection_model_compare/working'\n",
    "WEIGHTS_PATH = 'D:/Projetos/Mestrado/2024_Topicos_Esp_Sist_Informacao/ARTIGO_FINAL/object_detection_model_compare/weigths/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'\n",
    "\n",
    "\n",
    "class FilteredCocoDataset(Dataset):\n",
    "    def __init__(self, root, annFile, transform=None, max_samples=1000):\n",
    "        \"\"\"\n",
    "        Initialize the COCO dataset\n",
    "        Args:\n",
    "            root (str): Root directory where images are downloaded to\n",
    "            annFile (str): Path to json annotation file\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            max_samples (int): Maximum number of samples to use\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Filter for person, cat, and dog categories\n",
    "        cat_ids = self.coco.getCatIds(catNms=['person', 'cat', 'dog'])\n",
    "        img_ids = []\n",
    "        for cat_id in cat_ids:\n",
    "            img_ids.extend(self.coco.getImgIds(catIds=[cat_id]))\n",
    "        \n",
    "        # Remove duplicates and limit to max_samples\n",
    "        self.img_ids = list(set(img_ids))[:max_samples]\n",
    "        \n",
    "        # Create category mapping\n",
    "        self.cat_mapping = {cat_id: idx + 1 for idx, cat_id in enumerate(cat_ids)}\n",
    "        \n",
    "        print(f\"Found {len(self.img_ids)} images\")\n",
    "        print(f\"Category mapping: {self.cat_mapping}\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(os.path.join(self.root, img_info['file_name'])).convert('RGB')\n",
    "        \n",
    "        # Get annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            if ann['category_id'] in self.cat_mapping:\n",
    "                bbox = ann['bbox']\n",
    "                boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "                labels.append(self.cat_mapping[ann['category_id']])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id])\n",
    "        }\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "def get_model(num_classes, weights_path):\n",
    "    # Initialize model without downloading weights\n",
    "    model = fasterrcnn_resnet50_fpn(weights=None)\n",
    "    \n",
    "    # Load pretrained weights from local file\n",
    "    if os.path.exists(weights_path):\n",
    "        print(f\"Loading pretrained weights from {weights_path}\")\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print(f\"Warning: Weights file not found at {weights_path}. Starting with random weights.\")\n",
    "    \n",
    "    # Modify the box predictor for our number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_ap(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision\n",
    "    \"\"\"\n",
    "    # Simplified AP calculation\n",
    "    if len(predictions) == 0 or len(targets) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(targets)\n",
    "    \n",
    "    for pred in predictions:\n",
    "        for target in targets:\n",
    "            if torch.all(torch.abs(pred - target) < 0.5):  # IoU threshold of 0.5\n",
    "                correct += 1\n",
    "                break\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n",
    "            for images, targets in pbar:\n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += losses.item()\n",
    "                pbar.set_postfix({'loss': losses.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += losses.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1} - Training Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def calculate_map(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            predictions = model(images)\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            all_targets.extend(targets)\n",
    "    \n",
    "    # Calculate mAP (simplified version)\n",
    "    map_score = 0\n",
    "    num_classes = 4  # background + 3 classes\n",
    "    \n",
    "    for cls in range(1, num_classes):\n",
    "        predictions = [pred['boxes'][pred['labels'] == cls] for pred in all_predictions]\n",
    "        targets = [target['boxes'][target['labels'] == cls] for target in all_targets]\n",
    "        \n",
    "        # Calculate AP for each class\n",
    "        ap = calculate_ap(predictions, targets)\n",
    "        map_score += ap\n",
    "    \n",
    "    return map_score / (num_classes - 1)\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, map_score, save_dir):\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'loss_plot.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot mAP\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(['mAP'], [map_score])\n",
    "    plt.title('Mean Average Precision')\n",
    "    plt.ylabel('Score')\n",
    "    plt.savefig(os.path.join(save_dir, 'map_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = FilteredCocoDataset(\n",
    "        TRAIN_PATH,\n",
    "        os.path.join(ANNOTATIONS_PATH, 'instances_train2017.json'),\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = FilteredCocoDataset(\n",
    "        VAL_PATH,\n",
    "        os.path.join(ANNOTATIONS_PATH, 'instances_val2017.json'),\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: tuple(zip(*x))\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: tuple(zip(*x))\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    model = get_model(num_classes=4, weights_path=WEIGHTS_PATH)  # background + 3 classes\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, device)\n",
    "    \n",
    "    # Calculate mAP\n",
    "    print(\"Calculating mAP...\")\n",
    "    map_score = calculate_map(model, val_loader, device)\n",
    "    \n",
    "    # Plot and save metrics\n",
    "    print(\"Saving metrics and plots...\")\n",
    "    plot_metrics(train_losses, val_losses, map_score, WORKING_DIR)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), os.path.join(WORKING_DIR, 'faster_rcnn_model.pth'))\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6332084,
     "sourceId": 10239510,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
