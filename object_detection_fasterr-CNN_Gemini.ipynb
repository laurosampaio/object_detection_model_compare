{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "loading annotations into memory...\n",
      "Done (t=7.73s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lauro\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lauro\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(WORKING_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 85\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m train_annotations[i]:\n\u001b[0;32m     84\u001b[0m     boxes\u001b[38;5;241m.\u001b[39mappend(ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 85\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(CLASSES\u001b[38;5;241m.\u001b[39mindex(\u001b[43mcoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadCats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# COCO category IDs are not sequential\u001b[39;00m\n\u001b[0;32m     86\u001b[0m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(boxes)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     87\u001b[0m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from pycocotools.coco import COCO as coco\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths\n",
    "# TRAIN_PATH = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n",
    "# VAL_PATH = '/kaggle/input/coco-2017-dataset/coco2017/val2017'\n",
    "# ANNOTATIONS_PATH = '/kaggle/input/coco-2017-dataset/coco2017/annotations'\n",
    "# WORKING_DIR = '/kaggle/working'\n",
    "\n",
    "TRAIN_PATH = 'D:/Download/JDownloader/MSCOCO/images/train2017'\n",
    "VAL_PATH = 'D:/Download/JDownloader/MSCOCO/images/val2017'\n",
    "ANNOTATIONS_PATH = 'D:/Download/JDownloader/MSCOCO/annotations'\n",
    "WORKING_DIR = 'D:/Projetos/Mestrado/2024_Topicos_Esp_Sist_Informacao/ARTIGO_FINAL/object_detection_model_compare/working'\n",
    "\n",
    "\n",
    "\n",
    "# Define classes of interest\n",
    "CLASSES = ['person', 'cat', 'dog']\n",
    "NUM_CLASSES = len(CLASSES) + 1  # +1 for background\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "def create_coco_dataset(image_dir, annotations_file, classes, max_images_per_class=1000):\n",
    "    coco = COCO(annotations_file)\n",
    "    images = []\n",
    "    annotations = []\n",
    "    img_id = 0\n",
    "    for class_name in classes:\n",
    "        cat_ids = coco.getCatIds(catNms=[class_name])\n",
    "        img_ids = coco.getImgIds(catIds=cat_ids)\n",
    "        img_data = coco.loadImgs(img_ids)\n",
    "        count = 0\n",
    "        for img in img_data:\n",
    "            ann_ids = coco.getAnnIds(imgIds=img['id'], catIds=cat_ids, iscrowd=None)\n",
    "            anns = coco.loadAnns(ann_ids)\n",
    "            if anns:  # Only include images with annotations for the target classes\n",
    "                images.append(os.path.join(image_dir, img['file_name']))\n",
    "                annotations.append(anns)\n",
    "                count +=1\n",
    "            if count >= max_images_per_class:\n",
    "                break\n",
    "    return images, annotations\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def main():\n",
    "    # Data preprocessing\n",
    "    train_images, train_annotations = create_coco_dataset(TRAIN_PATH, os.path.join(ANNOTATIONS_PATH, 'instances_train2017.json'), CLASSES)\n",
    "    val_images, val_annotations = create_coco_dataset(VAL_PATH, os.path.join(ANNOTATIONS_PATH, 'instances_val2017.json'), CLASSES)\n",
    "\n",
    "\n",
    "    # Model setup\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "    model.to(device)\n",
    "\n",
    "    # Training\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    num_epochs = 10  # Reduced for demonstration\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i in range(len(train_images)):\n",
    "            img = torchvision.io.read_image(train_images[i]).to(device).float()/255.0\n",
    "            target = {}\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            for ann in train_annotations[i]:\n",
    "                boxes.append(ann['bbox'])\n",
    "                labels.append(CLASSES.index(coco.loadCats(ann['category_id'])[0]['name'])+1) # COCO category IDs are not sequential\n",
    "            target['boxes'] = torch.tensor(boxes).reshape(-1,4).to(device)\n",
    "            target['labels'] = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "            target[\"image_id\"] = torch.tensor([i])\n",
    "            target[\"area\"] = torch.tensor([ann['area'] for ann in train_annotations[i]])\n",
    "            target[\"iscrowd\"] = torch.tensor([ann['iscrowd'] for ann in train_annotations[i]], dtype=torch.int64)\n",
    "            loss_dict = model([img], [target])\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss/len(train_images))\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_images)}\")\n",
    "\n",
    "\n",
    "    # Saving outputs\n",
    "    torch.save(model.state_dict(), os.path.join(WORKING_DIR, 'faster_rcnn_model.pth'))\n",
    "\n",
    "    # Visualizations (Simplified for demonstration. Proper evaluation requires more complex metrics)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.savefig(os.path.join(WORKING_DIR, 'loss.png'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6332084,
     "sourceId": 10239510,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
