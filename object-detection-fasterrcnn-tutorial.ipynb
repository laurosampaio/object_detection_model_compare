{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3020523,"sourceType":"datasetVersion","datasetId":1849949}],"dockerImageVersionId":30155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Object Detection with Faster RCNN\n\nCode is for the following video: https://www.youtube.com/watch?v=Uc90rr5jbA4&t=71s\n\nDo give this notebook a thumbs-up if you liked it. Thanks!","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-30T12:21:50.551694Z","iopub.execute_input":"2024-12-30T12:21:50.551904Z","iopub.status.idle":"2024-12-30T12:21:50.574896Z","shell.execute_reply.started":"2024-12-30T12:21:50.551844Z","shell.execute_reply":"2024-12-30T12:21:50.574129Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"We require the latest version of torchvision","metadata":{}},{"cell_type":"code","source":"!pip install -U torchvision # We need a new versino of torchvision for this project","metadata":{"execution":{"iopub.status.busy":"2024-12-30T12:21:50.576685Z","iopub.execute_input":"2024-12-30T12:21:50.577453Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.10.1)\nCollecting torchvision\n  Downloading torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n     |████████████████████████████████| 24.2 MB 5.7 MB/s            \n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.19.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (8.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.26.0)\nCollecting torch==1.13.1\n  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n     |████████████████████████████████| 887.5 MB 6.6 kB/s              \n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision) (3.10.0.2)\nCollecting nvidia-cuda-runtime-cu11==11.7.99\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n     |████████████████████████████████| 849 kB 42.8 MB/s            \n\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n     |████████████████████████████████| 317.1 MB 25 kB/s               \n\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n     |████████████████████████████████| 21.0 MB 60.5 MB/s            \n\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n     |████████████████████████████████| 557.1 MB 3.3 kB/s              \n\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (0.37.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (59.4.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.0.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2021.10.8)\nInstalling collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, torch, torchvision\n  Attempting uninstall: torch\n    Found existing installation: torch 1.9.1\n    Uninstalling torch-1.9.1:\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Here are all the necessary libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import datasets, models\nfrom torchvision.transforms import functional as FT\nfrom torchvision import transforms as T\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, sampler, random_split, Dataset\nimport copy\nimport math\nfrom PIL import Image\nimport cv2\nimport albumentations as A  # our data augmentation library\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# remove arnings (optional)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom collections import defaultdict, deque\nimport datetime\nimport time\nfrom tqdm import tqdm # progress bar\nfrom torchvision.utils import draw_bounding_boxes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.__version__)\nprint(torchvision.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PyCOCOTools provides many utilities for dealing with datasets in the COCO format, and if you wanted, you could evaluate the model's performance on the dataset with some of the utilities provided with this library.\n\nThat is out of scope for this notebook, however.","metadata":{}},{"cell_type":"code","source":"# our dataset is in cocoformat, we will need pypcoco tools\n!pip install pycocotools\nfrom pycocotools.coco import COCO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now, we will define our transforms\nfrom albumentations.pytorch import ToTensorV2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use albumentations as our data augmentation library due to its capability to deal with bounding boxes in multiple formats","metadata":{}},{"cell_type":"code","source":"def get_transforms(train=False):\n    if train:\n        transform = A.Compose([\n            A.Resize(600, 600), # our input size can be 600px\n            A.HorizontalFlip(p=0.3),\n            A.VerticalFlip(p=0.3),\n            A.RandomBrightnessContrast(p=0.1),\n            A.ColorJitter(p=0.1),\n            ToTensorV2()\n        ], bbox_params=A.BboxParams(format='coco'))\n    else:\n        transform = A.Compose([\n            A.Resize(600, 600), # our input size can be 600px\n            ToTensorV2()\n        ], bbox_params=A.BboxParams(format='coco'))\n    return transform","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset\n\nThis is our dataset class. It loads all the necessary files and it processes the data so that it can be fed into the model.","metadata":{}},{"cell_type":"code","source":"class AquariumDetection(datasets.VisionDataset):\n    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n        # the 3 transform parameters are reuqired for datasets.VisionDataset\n        super().__init__(root, transforms, transform, target_transform)\n        self.split = split #train, valid, test\n        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\")) # annotatiosn stored here\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n    \n    def _load_image(self, id: int):\n        path = self.coco.loadImgs(id)[0]['file_name']\n        image = cv2.imread(os.path.join(self.root, self.split, path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n    def _load_target(self, id):\n        return self.coco.loadAnns(self.coco.getAnnIds(id))\n    \n    def __getitem__(self, index):\n        id = self.ids[index]\n        image = self._load_image(id)\n        target = self._load_target(id)\n        target = copy.deepcopy(self._load_target(id))\n        \n        boxes = [t['bbox'] + [t['category_id']] for t in target] # required annotation format for albumentations\n        if self.transforms is not None:\n            transformed = self.transforms(image=image, bboxes=boxes)\n        \n        image = transformed['image']\n        boxes = transformed['bboxes']\n        \n        new_boxes = [] # convert from xywh to xyxy\n        for box in boxes:\n            xmin = box[0]\n            xmax = xmin + box[2]\n            ymin = box[1]\n            ymax = ymin + box[3]\n            new_boxes.append([xmin, ymin, xmax, ymax])\n        \n        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n        \n        targ = {} # here is our transformed target\n        targ['boxes'] = boxes\n        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n        return image.div(255), targ # scale images\n    def __len__(self):\n        return len(self.ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/aquarium-dataset/Aquarium Combined/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load classes\ncoco = COCO(os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"))\ncategories = coco.cats\nn_classes = len(categories.keys())\ncategories","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code just gets a list of classes","metadata":{}},{"cell_type":"code","source":"classes = [i[1]['name'] for i in categories.items()]\nclasses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = AquariumDetection(root=dataset_path, transforms=get_transforms(True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is a sample image and its bounding boxes, this code does not get the model's output","metadata":{}},{"cell_type":"code","source":"# Lets view a sample\nsample = train_dataset[2]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\nplt.imshow(draw_bounding_boxes(\n    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n).permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model\n\nOur model is FasterRCNN with a backbone of `MobileNetV3-Large`. We need to change the output layers because we have just 7 classes but this model was trained on 90 classes.","metadata":{}},{"cell_type":"code","source":"# lets load the faster rcnn model\nmodel = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\nin_features = model.roi_heads.box_predictor.cls_score.in_features # we need to change the head\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is our collating function for the train dataloader, it allows us to create batches of data that can be easily pass into the model","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following blocks ensures that the model can take in the data and that it will not crash during training","metadata":{}},{"cell_type":"code","source":"images,targets = next(iter(train_loader))\nimages = list(image for image in images)\ntargets = [{k:v for k, v in t.items()} for t in targets]\noutput = model(images, targets) # just make sure this runs without error","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\") # use GPU to train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimizer\n\nHere, we define the optimizer. If you wish, you can also define the LR Scheduler, but it is not necessary for this notebook since our dataset is so small.\n\n> Note, there are a few bugs with the current way `lr_scheduler` is implemented. If you wish to use the scheduler, you will have to fix those bugs","metadata":{}},{"cell_type":"code","source":"# Now, and optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[16, 22], gamma=0.1) # lr scheduler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training\n\nThe following is a function that will train the model for one epoch. Torchvision Object Detections models have a loss function built in, and it will calculate the loss automatically if you pass in the `inputs` and `targets`","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, loader, device, epoch):\n    model.to(device)\n    model.train()\n    \n#     lr_scheduler = None\n#     if epoch == 0:\n#         warmup_factor = 1.0 / 1000 # do lr warmup\n#         warmup_iters = min(1000, len(loader) - 1)\n        \n#         lr_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor = warmup_factor, total_iters=warmup_iters)\n    \n    all_losses = []\n    all_losses_dict = []\n    \n    for images, targets in tqdm(loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets) # the model computes the loss automatically if we pass in targets\n        losses = sum(loss for loss in loss_dict.values())\n        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n        loss_value = losses.item()\n        \n        all_losses.append(loss_value)\n        all_losses_dict.append(loss_dict_append)\n        \n        if not math.isfinite(loss_value):\n            print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n            print(loss_dict)\n            sys.exit(1)\n        \n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n#         if lr_scheduler is not None:\n#             lr_scheduler.step() # \n        \n    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n        all_losses_dict['loss_classifier'].mean(),\n        all_losses_dict['loss_box_reg'].mean(),\n        all_losses_dict['loss_rpn_box_reg'].mean(),\n        all_losses_dict['loss_objectness'].mean()\n    ))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"10 Epochs should be enough to train this model for a high accuracy","metadata":{}},{"cell_type":"code","source":"num_epochs=10\n\nfor epoch in range(num_epochs):\n    train_one_epoch(model, optimizer, train_loader, device, epoch)\n#     lr_scheduler.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# our learning rate was too low, due to a lr scheduler bug. For this task, we wont need a scheudul.er","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Trying on sample Images\n\nThis is the inference code for the model. First, we set the model to evaluation mode and clear the GPU Cache. We also load a test dataset, so that we can use fresh images that the model hasn't seen.","metadata":{}},{"cell_type":"code","source":"# we will watch first epoich to ensure no errrors\n# while it is training, lets write code to see the models predictions. lets try again\nmodel.eval()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = AquariumDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, _ = test_dataset[5]\nimg_int = torch.tensor(img*255, dtype=torch.uint8)\nwith torch.no_grad():\n    prediction = model([img.to(device)])\n    pred = prediction[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# it did learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(14, 10))\nplt.imshow(draw_bounding_boxes(img_int,\n    pred['boxes'][pred['scores'] > 0.8],\n    [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n).permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}