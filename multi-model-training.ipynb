{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from pycocotools.coco import COCO\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, auc , precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU devices found\n"
     ]
    }
   ],
   "source": [
    "# Enable GPU acceleration\n",
    "def configure_gpu():\n",
    "    try:\n",
    "        # List physical devices\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        \n",
    "        if physical_devices:\n",
    "            # Print available GPUs\n",
    "            print(f\"Found {len(physical_devices)} GPU(s):\")\n",
    "            for device in physical_devices:\n",
    "                print(f\" - {device}\")\n",
    "                \n",
    "            # Important: Set memory growth BEFORE setting any other GPU configurations\n",
    "            try:\n",
    "                for device in physical_devices:\n",
    "                    tf.config.experimental.set_memory_growth(device, True)\n",
    "                print(\"Memory growth enabled on all GPUs\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Warning: {e}\")\n",
    "                \n",
    "            # Enable mixed precision after memory growth configuration\n",
    "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "            print(\"Enabled mixed precision training\")\n",
    "        else:\n",
    "            print(\"No GPU devices found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "\n",
    "\n",
    "configure_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for the dataset, anotations, working directory and images\n",
    "TRAIN_PATH = 'D:/Download/JDownloader/MSCOCO/images/train2017'\n",
    "VAL_PATH = 'D:/Download/JDownloader/MSCOCO/images/val2017'\n",
    "ANNOTATIONS_PATH = 'D:/Download/JDownloader/MSCOCO/annotations'\n",
    "WORKING_DIR = 'D:/Projetos/Mestrado/2024_Topicos_Esp_Sist_Informacao/ARTIGO_FINAL/object_detection_model_compare/working'\n",
    "VAL_MODEL_IMG = 'D:/Projetos/Mestrado/2024_Topicos_Esp_Sist_Informacao/ARTIGO_FINAL/object_detection_model_compare/val_model_img'\n",
    "FILTERED_CATEGORIES = ['person', 'cat', 'dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.78s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Load COCO annotations\n",
    "annotations_file = os.path.join(ANNOTATIONS_PATH, 'instances_train2017.json')\n",
    "coco = COCO(annotations_file)\n",
    "\n",
    "# Get category IDs for the selected categories\n",
    "category_ids = coco.getCatIds(catNms=FILTERED_CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get total image count per category\n",
    "# category_image_counts = {}\n",
    "\n",
    "# for category_name, category_id in zip(FILTERED_CATEGORIES, category_ids):\n",
    "#     # Get all annotation IDs for the category\n",
    "#     ann_ids = coco.getAnnIds(catIds=[category_id])\n",
    "    \n",
    "#     # Load annotations and extract unique image IDs\n",
    "#     anns = coco.loadAnns(ann_ids)\n",
    "#     image_ids = {ann['image_id'] for ann in anns}  # Use a set to ensure uniqueness\n",
    "    \n",
    "#     # Count unique images\n",
    "#     category_image_counts[category_name] = len(image_ids)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Total image count per category:\")\n",
    "# for category, count in category_image_counts.items():\n",
    "#     print(f\"{category}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate filtered dataset to train the model\n",
    "\n",
    "# # Collect up to 1000 annotations per category\n",
    "# filtered_data = []\n",
    "# for category_id in category_ids:\n",
    "#     ann_ids = coco.getAnnIds(catIds=[category_id])\n",
    "#     anns = coco.loadAnns(ann_ids)\n",
    "#     selected_anns = random.sample(anns, min(1000, len(anns)))\n",
    "#     for ann in selected_anns:\n",
    "#         image_info = coco.loadImgs(ann['image_id'])[0]\n",
    "#         filtered_data.append({\n",
    "#             \"image_id\": ann['image_id'],\n",
    "#             \"image\": image_info['file_name'],\n",
    "#             \"category_id\": ann['category_id']\n",
    "#         })\n",
    "\n",
    "# # Save filtered data to CSV\n",
    "# filtered_csv_path = os.path.join(WORKING_DIR, 'filtered_coco.csv')\n",
    "# filtered_df = pd.DataFrame(filtered_data)\n",
    "# filtered_df.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "# print(f\"Filtered dataset saved to {os.path.abspath(filtered_csv_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the filtered dataset\n",
    "# # Load the filtered CSV\n",
    "# data = pd.read_csv(filtered_csv_path)\n",
    "\n",
    "# # Display record count per category_id\n",
    "# category_counts = data['category_id'].value_counts()\n",
    "# print(\"Record count per category_id:\")\n",
    "# print(category_counts)\n",
    "# print(\"\")\n",
    "\n",
    "# # Split into training and testing datasets\n",
    "# train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Display record count per category_id train and test\n",
    "# category_train_counts = train_data['category_id'].value_counts()\n",
    "# print(\"Train - Record count per category_id:\")\n",
    "# print(category_train_counts)\n",
    "# print(\"\")\n",
    "\n",
    "# category_test_counts = test_data['category_id'].value_counts()\n",
    "# print(\"Test - Record count per category_id:\")\n",
    "# print(category_test_counts)\n",
    "# print(\"\")\n",
    "\n",
    "# # Save the split datasets\n",
    "# train_csv_path = os.path.join(WORKING_DIR, 'train_data.csv')\n",
    "# test_csv_path = os.path.join(WORKING_DIR, 'test_data.csv')\n",
    "\n",
    "# train_data.to_csv(train_csv_path, index=False)\n",
    "# test_data.to_csv(test_csv_path, index=False)\n",
    "\n",
    "# print(f\"Training dataset saved to {os.path.abspath(train_csv_path)}\")\n",
    "# print(f\"Testing dataset saved to {os.path.abspath(test_csv_path)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split datasets\n",
    "train_csv_path = os.path.join(WORKING_DIR, 'train_data.csv')\n",
    "test_csv_path = os.path.join(WORKING_DIR, 'test_data.csv')\n",
    "\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "test_data = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Load and preprocess an image for MobileNetV2.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Load images and labels\n",
    "def load_data(data, path_prefix, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for _, row in data.iterrows():\n",
    "        image_path = os.path.join(path_prefix, row['image'])\n",
    "        images.append(preprocess_image(image_path, target_size))\n",
    "        labels.append(row['category_id'])\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "X_train, y_train = load_data(train_data, TRAIN_PATH)\n",
    "X_test, y_test = load_data(test_data, TRAIN_PATH)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = to_categorical(label_encoder.fit_transform(y_train))\n",
    "y_test_encoded = to_categorical(label_encoder.transform(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m327,936\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,619,203</span> (9.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,619,203\u001b[0m (9.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,585,091</span> (9.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,585,091\u001b[0m (9.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,112</span> (133.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m34,112\u001b[0m (133.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary saved as an image at D:/Projetos/Mestrado/2024_Topicos_Esp_Sist_Informacao/ARTIGO_FINAL/object_detection_model_compare/working\\model_summary_20241224_185453.png\n"
     ]
    }
   ],
   "source": [
    "# Build the model using MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Unfreeze some of the later layers for fine-tuning\n",
    "for layer in base_model.layers[-30:]:  # Unfreeze last 30 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add improved classification head\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs)\n",
    "x = GlobalAveragePooling2D()(x)  # Use GAP instead of Flatten\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile with a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Get current datetime and format it\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Define file path for the model summary\n",
    "model_summary_path = os.path.join(WORKING_DIR, f'model_summary_{timestamp}.png')\n",
    "\n",
    "# Save the model summary as an image\n",
    "plot_model(model, to_file=model_summary_path, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "print(f\"Model summary saved as an image at {model_summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lauro\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.4699 - loss: 6.9633 - val_accuracy: 0.7550 - val_loss: 6.2293\n",
      "Epoch 2/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.7223 - loss: 6.1745 - val_accuracy: 0.7700 - val_loss: 5.9036\n",
      "Epoch 3/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.7938 - loss: 5.6727 - val_accuracy: 0.7917 - val_loss: 5.5504\n",
      "Epoch 4/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.8209 - loss: 5.2647 - val_accuracy: 0.7817 - val_loss: 5.1881\n",
      "Epoch 5/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.8474 - loss: 4.8781 - val_accuracy: 0.8017 - val_loss: 4.8643\n",
      "Epoch 6/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.8497 - loss: 4.5065 - val_accuracy: 0.8133 - val_loss: 4.5159\n",
      "Epoch 7/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.8994 - loss: 4.1107 - val_accuracy: 0.7733 - val_loss: 4.2940\n",
      "Epoch 8/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.9021 - loss: 3.7921 - val_accuracy: 0.7650 - val_loss: 4.1412\n",
      "Epoch 9/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.9178 - loss: 3.5108 - val_accuracy: 0.7733 - val_loss: 3.7932\n",
      "Epoch 10/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.9274 - loss: 3.2180 - val_accuracy: 0.7833 - val_loss: 3.6475\n",
      "Epoch 11/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.9398 - loss: 2.9596 - val_accuracy: 0.7717 - val_loss: 3.4799\n",
      "Epoch 12/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.9413 - loss: 2.7217 - val_accuracy: 0.7883 - val_loss: 3.3312\n",
      "Epoch 13/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.9559 - loss: 2.4756 - val_accuracy: 0.7767 - val_loss: 3.1839\n",
      "Epoch 14/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.9521 - loss: 2.2951 - val_accuracy: 0.7917 - val_loss: 2.9929\n",
      "Epoch 15/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.9614 - loss: 2.0771 - val_accuracy: 0.7467 - val_loss: 3.1473\n",
      "Epoch 16/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.9599 - loss: 1.9381 - val_accuracy: 0.7483 - val_loss: 2.9977\n",
      "Epoch 17/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.9727 - loss: 1.7389 - val_accuracy: 0.7717 - val_loss: 2.7925\n",
      "Epoch 18/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - accuracy: 0.9686 - loss: 1.6081 - val_accuracy: 0.7700 - val_loss: 2.5048\n",
      "Epoch 19/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.9704 - loss: 1.4678 - val_accuracy: 0.7900 - val_loss: 2.2273\n",
      "Epoch 20/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.9738 - loss: 1.3364 - val_accuracy: 0.7633 - val_loss: 2.5208\n",
      "Epoch 21/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.9735 - loss: 1.2276 - val_accuracy: 0.7833 - val_loss: 2.1382\n",
      "Epoch 22/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.9739 - loss: 1.1223 - val_accuracy: 0.7800 - val_loss: 2.1574\n",
      "Epoch 23/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.9737 - loss: 1.0239 - val_accuracy: 0.7950 - val_loss: 1.8646\n",
      "Epoch 24/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.9731 - loss: 0.9487 - val_accuracy: 0.7917 - val_loss: 1.7899\n",
      "Epoch 25/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 1s/step - accuracy: 0.9826 - loss: 0.8501 - val_accuracy: 0.8050 - val_loss: 1.6880\n",
      "Epoch 26/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 0.9787 - loss: 0.7990 - val_accuracy: 0.7900 - val_loss: 1.6210\n",
      "Epoch 27/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.9832 - loss: 0.7015 - val_accuracy: 0.7800 - val_loss: 1.6887\n",
      "Epoch 28/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 0.9861 - loss: 0.6376 - val_accuracy: 0.7817 - val_loss: 1.5738\n",
      "Epoch 29/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - accuracy: 0.9741 - loss: 0.6154 - val_accuracy: 0.7617 - val_loss: 1.5969\n",
      "Epoch 30/30\n",
      "\u001b[1m32/75\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.9793 - loss: 0.5457"
     ]
    }
   ],
   "source": [
    "# Get current timestamp for file naming\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Add callbacks for better training\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train_encoded, batch_size=32),\n",
    "    validation_data=(X_test, y_test_encoded),\n",
    "    epochs=30,  # Increase epochs since we have early stopping\n",
    "   # callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "training_duration = str(timedelta(seconds=training_time))\n",
    "\n",
    "# Print training time\n",
    "print(\"\\nModel Training Time:\")\n",
    "print(f\"Total seconds: {training_time:.2f} seconds\")\n",
    "print(f\"Formatted duration: {training_duration}\")\n",
    "print(f\"Average time per epoch: {(training_time/len(history.history['loss'])):.2f} seconds\")\n",
    "\n",
    "# Create log filename with timestamp\n",
    "log_filename = f'training_log_{timestamp}.txt'\n",
    "log_filepath = os.path.join(WORKING_DIR, log_filename)\n",
    "\n",
    "# Save detailed training log\n",
    "with open(log_filepath, 'w') as f:\n",
    "    f.write(f\"Training Log - {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Training Time Details:\\n\")\n",
    "    f.write(\"-\"*20 + \"\\n\")\n",
    "    f.write(f\"Start Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\\n\")\n",
    "    f.write(f\"End Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\\n\")\n",
    "    f.write(f\"Total Training Time: {training_duration}\\n\")\n",
    "    f.write(f\"Total Seconds: {training_time:.2f}\\n\")\n",
    "    f.write(f\"Average Time per Epoch: {(training_time/len(history.history['loss'])):.2f} seconds\\n\\n\")\n",
    "    \n",
    "    f.write(\"Training Parameters:\\n\")\n",
    "    f.write(\"-\"*20 + \"\\n\")\n",
    "    f.write(f\"Number of Epochs: {len(history.history['loss'])}\\n\")\n",
    "    f.write(f\"Batch Size: 32\\n\")\n",
    "    f.write(f\"Training Samples: {len(X_train)}\\n\")\n",
    "    f.write(f\"Validation Samples: {len(X_test)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Final Model Performance:\\n\")\n",
    "    f.write(\"-\"*20 + \"\\n\")\n",
    "    f.write(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\\n\")\n",
    "\n",
    "print(f\"\\nTraining log saved to: {log_filepath}\")\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train, y_train_encoded,\n",
    "#     validation_data=(X_test, y_test_encoded),\n",
    "#     epochs=30,  # Adjust epochs based on performance\n",
    "#     batch_size=32,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = os.path.join(WORKING_DIR, 'mobilenet_v2_coco.keras')\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"Trained model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current datetime and format it\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Define file paths with timestamp\n",
    "accuracy_loss_plot_path = os.path.join(WORKING_DIR, f'accuracy_loss_plot_{timestamp}.png')\n",
    "roc_curve_plot_path = os.path.join(WORKING_DIR, f'roc_curve_plot_{timestamp}.png')\n",
    "\n",
    "# Plot training accuracy and loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save accuracy and loss plot with timestamp\n",
    "plt.savefig(accuracy_loss_plot_path, dpi=300)  # Save with high resolution\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy and loss plot saved at {accuracy_loss_plot_path}\")\n",
    "\n",
    "# ROC Curve\n",
    "y_test_pred = model.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test_encoded.ravel(), y_test_pred.ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Save ROC curve plot with timestamp\n",
    "plt.savefig(roc_curve_plot_path, dpi=300)  # Save with high resolution\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC Curve plot saved at {roc_curve_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model precision-recall , average precision and mAP\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test_encoded, axis=1)\n",
    "\n",
    "# Calculate precision-recall and average precision for each class\n",
    "\n",
    "\n",
    "# Number of classes\n",
    "num_classes = y_test_encoded.shape[1]\n",
    "\n",
    "# Store AP values for each class\n",
    "average_precisions = {}\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(num_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_encoded[:, i], y_pred[:, i])\n",
    "    ap = average_precision_score(y_test_encoded[:, i], y_pred[:, i])\n",
    "    average_precisions[i] = ap\n",
    "\n",
    "    # Plot PR curve for each class\n",
    "    plt.plot(recall, precision, label=f\"Class {label_encoder.classes_[i]} (AP={ap:.2f})\")\n",
    "\n",
    "# Calculate mAP\n",
    "mAP = np.mean(list(average_precisions.values()))\n",
    "print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "\n",
    "# Finalize PR plot\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the PR curve plot\n",
    "pr_curve_path = os.path.join(WORKING_DIR, f'pr_curve_{timestamp}.png')\n",
    "plt.savefig(pr_curve_path)\n",
    "plt.show()\n",
    "print(f\"Precision-Recall curve saved at {pr_curve_path}\")\n",
    "\n",
    "# Compare with another model (example usage)\n",
    "# Assuming you have predictions from another model in `y_pred_other`\n",
    "# y_pred_other = ...\n",
    "\n",
    "# Uncomment the below lines to compare:\n",
    "# average_precisions_other = {}\n",
    "# for i in range(num_classes):\n",
    "#     precision, recall, _ = precision_recall_curve(y_test_encoded[:, i], y_pred_other[:, i])\n",
    "#     ap = average_precision_score(y_test_encoded[:, i], y_pred_other[:, i])\n",
    "#     average_precisions_other[i] = ap\n",
    "#\n",
    "# mAP_other = np.mean(list(average_precisions_other.values()))\n",
    "# print(f\"Comparison Model mAP: {mAP_other:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_path = os.path.join(WORKING_DIR, 'mobilenet_v2_coco.keras')\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO validation annotations\n",
    "val_annotations_file = os.path.join(ANNOTATIONS_PATH, 'instances_val2017.json')\n",
    "coco_val = COCO(val_annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict category for an image, now measuring inference time\n",
    "def predict_category_with_time(image_path, model, target_size=(224, 224)):\n",
    "    image = preprocess_image(image_path, target_size)\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(image)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    confidence = predictions[0][predicted_class]\n",
    "    return predicted_class, confidence, inference_time\n",
    "\n",
    "# Function to display image with prediction results and inference time\n",
    "def display_image_with_prediction(image_path, predicted_label, confidence, inference_time):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB for display\n",
    "    true_category = coco_val.cats[predicted_label]['name']  \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"True: {true_category}\\n\"\n",
    "              f\"Predicted: {predicted_label} ({confidence*100:.2f}%)\\n\"\n",
    "              f\"Inference Time: {inference_time*1000:.2f} ms\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model with a few images\n",
    "image_filenames = ['000000205834.jpg', '000000050811.jpg', '000000058111.jpg']\n",
    "\n",
    "# Create full paths for the images\n",
    "validation_images = {filename: os.path.join(VAL_MODEL_IMG, filename) for filename in image_filenames}\n",
    "\n",
    "# Perform predictions and display images with inference time\n",
    "for filename, image_path in validation_images.items():\n",
    "    predicted_class, confidence, inference_time = predict_category_with_time(image_path, loaded_model)\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    # Display image with prediction results and inference time\n",
    "    display_image_with_prediction(image_path, predicted_label, confidence, inference_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Validation Dataset to infer medium acuracy and time \n",
    "\n",
    "# Collect up to 280 annotations per category\n",
    "val_filtered_data = []\n",
    "for category_id in category_ids:\n",
    "    ann_ids = coco_val.getAnnIds(catIds=[category_id])\n",
    "    anns = coco_val.loadAnns(ann_ids)\n",
    "    selected_anns = random.sample(anns, min(280, len(anns)))\n",
    "    for ann in selected_anns:\n",
    "        image_info = coco_val.loadImgs(ann['image_id'])[0]\n",
    "        val_filtered_data.append({\n",
    "            \"image_id\": ann['image_id'],\n",
    "            \"image\": image_info['file_name'],\n",
    "            \"category_id\": ann['category_id']\n",
    "        })\n",
    "\n",
    "# Save filtered data to CSV\n",
    "val_filtered_csv_path = os.path.join(WORKING_DIR, 'val_filtered_data.csv')\n",
    "val_filtered_df = pd.DataFrame(val_filtered_data)\n",
    "val_filtered_df.to_csv(val_filtered_csv_path, index=False)\n",
    "\n",
    "print(f\"Validation Filtered dataset saved to {os.path.abspath(val_filtered_csv_path)}\")\n",
    "\n",
    "\n",
    "# Load the filtered CSV\n",
    "val_data = pd.read_csv(val_filtered_csv_path)\n",
    "\n",
    "# Display record count per category_id to validate\n",
    "val_category_counts = val_data['category_id'].value_counts()\n",
    "print(\"Record count per category_id:\")\n",
    "print(val_category_counts)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns to val_data\n",
    "val_data['predicted_class'] = None\n",
    "val_data['confidence'] = None\n",
    "val_data['inference_time'] = None\n",
    "\n",
    "# Process each record in val_data\n",
    "for index, row in val_data.iterrows():\n",
    "    image_path = os.path.join(VAL_PATH, row['image'])  # Construct full image path    \n",
    "    predicted_class, confidence, inference_time = predict_category_with_time(image_path, loaded_model)\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    # Save results to val_data\n",
    "    val_data.at[index, 'predicted_class'] = predicted_label\n",
    "    val_data.at[index, 'confidence'] = round(confidence * 100, 2)\n",
    "    val_data.at[index, 'inference_time'] = inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered data to CSV\n",
    "val_data_inference_csv_path = os.path.join(WORKING_DIR, 'val_data_inference.csv')\n",
    "val_data_inference = pd.DataFrame(val_data)\n",
    "val_data_inference.to_csv(val_data_inference_csv_path, index=False)\n",
    "\n",
    "print(f\"Validation Filtered dataset saved to {os.path.abspath(val_data_inference_csv_path)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display averages\n",
    "average_confidence = val_data['confidence'].mean()\n",
    "average_inference_time = val_data['inference_time'].mean()\n",
    "\n",
    "print(f\"Average Confidence: {average_confidence:.2f}%\")\n",
    "print(f\"Average Inference Time: {average_inference_time*1000:.2f} ms\")\n",
    "print(\"\")\n",
    "print(f\"Total Validation Records: {val_data.shape[0]}\")\n",
    "print(\"\")\n",
    "\n",
    "# Identify mismatches\n",
    "mismatched_records = val_data[val_data['predicted_class'] != val_data['category_id']]\n",
    "print(f\"Records where predicted_class differs from category_id: {mismatched_records.shape[0]}\")\n",
    "print(\"\")\n",
    "\n",
    "# Identify equals\n",
    "equals_records = val_data[val_data['predicted_class'] == val_data['category_id']]\n",
    "equals_count = equals_records.shape[0]\n",
    "print(f\"Records where predicted_class equals category_id: {equals_count}\")\n",
    "print(\"\")\n",
    "\n",
    "# Calculate percentage of matches\n",
    "total_records = val_data.shape[0]\n",
    "percentage_equals = (equals_count / total_records) * 100\n",
    "print(f\"Percentage of records where predicted_class equals category_id: {percentage_equals:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6332084,
     "sourceId": 10239510,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
